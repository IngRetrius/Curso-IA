{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL - Base de Datos de Empresas Activas\n",
    "## Camara de Comercio de Ibague - Corte a 31 de Diciembre de 2025\n",
    "\n",
    "**Autor:** Juan Camilo Perea Possos  \n",
    "**Fecha:** 20 de Febrero de 2026  \n",
    "\n",
    "---\n",
    "\n",
    "### Descripcion del proyecto\n",
    "\n",
    "Este notebook realiza un proceso completo de **ETL (Extract, Transform, Load)** sobre la base de datos publica de empresas y/o entidades activas registradas en la jurisdiccion de la Camara de Comercio de Ibague, con corte al 31 de diciembre de 2025.\n",
    "\n",
    "El proceso ETL se compone de tres fases fundamentales:\n",
    "\n",
    "1. **Extract (Extraccion):** Carga de los datos crudos desde el archivo CSV original.\n",
    "2. **Transform (Transformacion):** Limpieza, estandarizacion, conversion de tipos de datos, tratamiento de valores faltantes, analisis de correlacion y seleccion de variables relevantes.\n",
    "3. **Load (Carga):** Exportacion de los datos limpios y transformados a un nuevo archivo CSV.\n",
    "\n",
    "Adicionalmente, se incluye un **Analisis Exploratorio de Datos (EDA)** con visualizaciones que permiten comprender la estructura y distribucion de la informacion empresarial de la region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 1: Importacion de Librerias\n",
    "\n",
    "En este primer paso se importan todas las librerias necesarias para el desarrollo del ETL y el analisis exploratorio. Cada libreria cumple un rol especifico:\n",
    "\n",
    "- **pandas:** Manipulacion y analisis de datos tabulares (DataFrames).\n",
    "- **numpy:** Operaciones numericas y manejo de valores nulos (NaN).\n",
    "- **matplotlib:** Creacion de graficos y visualizaciones estaticas.\n",
    "- **seaborn:** Visualizaciones estadisticas de alto nivel, construida sobre matplotlib.\n",
    "- **wordcloud:** Generacion de nubes de palabras a partir de texto.\n",
    "- **warnings:** Supresion de advertencias menores para mantener el notebook limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 1: IMPORTACION DE LIBRERIAS\n",
    "# ============================================================\n",
    "\n",
    "# pandas: libreria principal para manipulacion de datos en formato tabular (filas y columnas)\n",
    "import pandas as pd\n",
    "\n",
    "# numpy: libreria para operaciones numericas y manejo de valores especiales como NaN\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib.pyplot: motor de graficacion para crear visualizaciones estaticas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn: libreria de visualizacion estadistica que extiende matplotlib con graficos mas elegantes\n",
    "import seaborn as sns\n",
    "\n",
    "# WordCloud: clase para generar nubes de palabras a partir de frecuencias de texto\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# warnings: modulo para controlar la visualizacion de advertencias del sistema\n",
    "import warnings\n",
    "\n",
    "# Suprimir advertencias menores que no afectan el resultado pero ensucian la salida\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar el estilo visual de seaborn para que los graficos tengan un fondo con cuadricula\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Configurar matplotlib para que los graficos se muestren directamente en el notebook\n",
    "%matplotlib inline\n",
    "\n",
    "print('Librerias importadas correctamente.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 2: Extraccion de Datos (Extract)\n",
    "\n",
    "La fase de **extraccion** consiste en leer los datos desde su fuente original. En este caso, el archivo CSV contiene 20,280 registros de empresas y entidades activas.\n",
    "\n",
    "Se utiliza `pd.read_csv()` especificando:\n",
    "- La ruta del archivo fuente.\n",
    "- La codificacion `utf-8` para manejar correctamente caracteres especiales (tildes, enes).\n",
    "- El separador de columnas (coma por defecto en CSV).\n",
    "\n",
    "Tras la carga, se realiza una inspeccion inicial para verificar que los datos se leyeron correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 2: EXTRACCION DE DATOS (EXTRACT)\n",
    "# ============================================================\n",
    "\n",
    "# Definir la ruta al archivo CSV original\n",
    "# Se usa raw string (r\"\") para evitar problemas con las barras invertidas en Windows\n",
    "ruta_archivo = r'data/BASE_DE_DATOS_DE_EMPRESAS_Y_O_ENTIDADES_ACTIVAS_-_JURISDICCIÓN_CÁMARA_DE_COMERCIO_DE_IBAGUÉ_-_CORTE_A_31_DE_DICIEMBRE_DE_2025_20260207.csv'\n",
    "\n",
    "# Leer el archivo CSV y cargarlo en un DataFrame de pandas\n",
    "# encoding='utf-8': asegura la correcta lectura de caracteres especiales (tildes, ñ)\n",
    "# sep=',': indica que las columnas estan separadas por comas\n",
    "df = pd.read_csv(ruta_archivo, encoding='utf-8', sep=',')\n",
    "\n",
    "# Mostrar las dimensiones del DataFrame (filas, columnas)\n",
    "print(f'Dimensiones del dataset: {df.shape[0]} filas x {df.shape[1]} columnas')\n",
    "print(f'Total de registros cargados: {df.shape[0]:,}')\n",
    "print(f'Total de variables (columnas): {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las primeras 5 filas para verificar que la carga fue correcta\n",
    "# .head() devuelve las primeras n filas del DataFrame (por defecto 5)\n",
    "print('--- Primeras 5 filas del dataset ---')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todas las columnas del dataset para conocer las variables disponibles\n",
    "# .columns devuelve un Index con los nombres de todas las columnas\n",
    "print('--- Lista completa de columnas ---')\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f'  {i:2d}. {col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 3: Exploracion Inicial del Dataset\n",
    "\n",
    "Antes de transformar los datos, es fundamental comprender su estructura. En esta fase se analiza:\n",
    "\n",
    "- **Tipos de datos** de cada columna (texto, numerico, fecha, etc.).\n",
    "- **Valores unicos** por columna para entender la cardinalidad.\n",
    "- **Estadisticas descriptivas** tanto para variables numericas como categoricas.\n",
    "- **Valores especiales** como \"No reporta\" y \"No aplica\" que actuan como datos faltantes enmascarados.\n",
    "\n",
    "Esta exploracion permite tomar decisiones informadas sobre que transformaciones aplicar en los siguientes pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 3: EXPLORACION INICIAL\n",
    "# ============================================================\n",
    "\n",
    "# .info() muestra el tipo de dato de cada columna, cantidad de valores no nulos\n",
    "# y el uso de memoria del DataFrame\n",
    "print('=== INFORMACION GENERAL DEL DATASET ===')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe() genera estadisticas descriptivas para las columnas numericas:\n",
    "# count (conteo), mean (media), std (desviacion estandar), min, max y percentiles\n",
    "print('=== ESTADISTICAS DESCRIPTIVAS - VARIABLES NUMERICAS ===')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe(include='object') genera estadisticas para columnas de texto:\n",
    "# count (conteo), unique (valores unicos), top (valor mas frecuente), freq (frecuencia del top)\n",
    "print('=== ESTADISTICAS DESCRIPTIVAS - VARIABLES CATEGORICAS ===')\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar cuantos valores \"No reporta\" y \"No aplica\" hay en cada columna\n",
    "# Estos valores son datos faltantes enmascarados como texto\n",
    "print('=== CONTEO DE VALORES ESPECIALES POR COLUMNA ===')\n",
    "print()\n",
    "\n",
    "# Iterar sobre cada columna del DataFrame\n",
    "for col in df.columns:\n",
    "    # Contar ocurrencias de \"No reporta\" en la columna actual\n",
    "    no_reporta = (df[col] == 'No reporta').sum()\n",
    "    # Contar ocurrencias de \"No aplica\" en la columna actual\n",
    "    no_aplica = (df[col] == 'No aplica').sum()\n",
    "    # Contar valores nulos reales (NaN)\n",
    "    nulos = df[col].isna().sum()\n",
    "    \n",
    "    # Solo mostrar columnas que tengan al menos uno de estos valores\n",
    "    if no_reporta > 0 or no_aplica > 0 or nulos > 0:\n",
    "        print(f'{col}:')\n",
    "        if no_reporta > 0:\n",
    "            print(f'    \"No reporta\": {no_reporta:,} ({no_reporta/len(df)*100:.1f}%)')\n",
    "        if no_aplica > 0:\n",
    "            print(f'    \"No aplica\":  {no_aplica:,} ({no_aplica/len(df)*100:.1f}%)')\n",
    "        if nulos > 0:\n",
    "            print(f'    NaN (nulos):  {nulos:,} ({nulos/len(df)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si existen filas duplicadas en el dataset\n",
    "# .duplicated() retorna True para cada fila que es duplicada de otra anterior\n",
    "duplicados = df.duplicated().sum()\n",
    "print(f'Cantidad de filas duplicadas: {duplicados}')\n",
    "\n",
    "# Verificar duplicados solo por la columna MATRICULA (identificador unico esperado)\n",
    "duplicados_matricula = df['MATRICULA'].duplicated().sum()\n",
    "print(f'Matriculas duplicadas: {duplicados_matricula}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 4: Transformacion - Limpieza de Datos (Transform)\n",
    "\n",
    "La fase de **transformacion** es el nucleo del proceso ETL. Aqui se realizan las siguientes operaciones:\n",
    "\n",
    "1. **Reemplazo de valores especiales:** Los valores \"No reporta\" y \"No aplica\" se convierten a `NaN` (Not a Number), que es el estandar de pandas para representar datos faltantes.\n",
    "2. **Eliminacion de duplicados:** Se eliminan filas completamente duplicadas si existen.\n",
    "3. **Conversion de tipos de datos:**\n",
    "   - Columnas con fechas en formato `YYYYMMDD` se convierten al tipo `datetime`.\n",
    "   - Columnas numericas almacenadas como texto se convierten a tipo numerico.\n",
    "4. **Limpieza de texto:** Estandarizacion de mayusculas/minusculas y eliminacion de espacios extra.\n",
    "5. **Eliminacion de columnas con exceso de datos faltantes:** Columnas con mas del 80% de valores nulos se consideran poco utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 4: TRANSFORMACION - LIMPIEZA DE DATOS\n",
    "# ============================================================\n",
    "\n",
    "# 4.1 - Crear una copia del DataFrame original para preservar los datos crudos\n",
    "# Esto es una buena practica: siempre trabajar sobre una copia para poder comparar\n",
    "df_limpio = df.copy()\n",
    "\n",
    "print(f'Shape antes de limpieza: {df_limpio.shape}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 - Reemplazar valores especiales por NaN\n",
    "# \"No reporta\" y \"No aplica\" no son datos reales sino indicadores de ausencia de informacion\n",
    "# Al convertirlos a NaN, pandas puede manejarlos correctamente en calculos y filtros\n",
    "\n",
    "# Lista de valores que representan datos faltantes enmascarados\n",
    "valores_faltantes = ['No reporta', 'No aplica', 'no reporta', 'no aplica', 'NO REPORTA', 'NO APLICA']\n",
    "\n",
    "# .replace() busca y reemplaza los valores especificados en todo el DataFrame\n",
    "# np.nan es el valor estandar de \"Not a Number\" en numpy/pandas\n",
    "df_limpio = df_limpio.replace(valores_faltantes, np.nan)\n",
    "\n",
    "# Verificar el resultado: contar nulos totales despues del reemplazo\n",
    "total_nulos = df_limpio.isna().sum().sum()\n",
    "total_celdas = df_limpio.shape[0] * df_limpio.shape[1]\n",
    "print(f'Total de celdas en el dataset: {total_celdas:,}')\n",
    "print(f'Total de valores nulos (NaN) despues del reemplazo: {total_nulos:,}')\n",
    "print(f'Porcentaje de datos faltantes: {total_nulos/total_celdas*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 - Eliminar filas completamente duplicadas\n",
    "# .drop_duplicates() elimina filas donde TODAS las columnas son identicas a otra fila\n",
    "filas_antes = len(df_limpio)\n",
    "df_limpio = df_limpio.drop_duplicates()\n",
    "filas_despues = len(df_limpio)\n",
    "\n",
    "print(f'Filas antes de eliminar duplicados: {filas_antes:,}')\n",
    "print(f'Filas despues de eliminar duplicados: {filas_despues:,}')\n",
    "print(f'Filas eliminadas: {filas_antes - filas_despues}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 - Conversion de columnas de fecha\n",
    "# Las fechas estan almacenadas como texto en formato YYYYMMDD (ej: \"20251230\")\n",
    "# Se convierten al tipo datetime de pandas para poder hacer operaciones temporales\n",
    "\n",
    "# Definir las columnas que contienen fechas en formato YYYYMMDD\n",
    "columnas_fecha = [\n",
    "    'FECHA DE MATRICULA', 'FECHA RENOVACION', 'FECHA CONSTITUCION',\n",
    "    'FECHA DE VIGENCIA', 'FECHA DATOS TAMAÑO EMPRESARIAL',\n",
    "    'FECHA DE DATOS NIIF'\n",
    "]\n",
    "\n",
    "for col in columnas_fecha:\n",
    "    if col in df_limpio.columns:\n",
    "        # pd.to_datetime() convierte texto a tipo fecha\n",
    "        # format='%Y%m%d': indica que el formato es AñoMesDia (ej: 20251230)\n",
    "        # errors='coerce': si un valor no se puede convertir, lo pone como NaT (Not a Time)\n",
    "        df_limpio[col] = pd.to_datetime(df_limpio[col], format='%Y%m%d', errors='coerce')\n",
    "        print(f'Columna \"{col}\" convertida a datetime')\n",
    "\n",
    "# Convertir tambien las columnas de fechas de pago de renovacion (2016-2025)\n",
    "for anio in range(2016, 2026):\n",
    "    col_pago = f'FECHA DE PAGO DE RENOVACION {anio}'\n",
    "    if col_pago in df_limpio.columns:\n",
    "        df_limpio[col_pago] = pd.to_datetime(df_limpio[col_pago], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "print(f'\\nColumnas de pago de renovacion (2016-2025) convertidas a datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 - Conversion de columnas numericas\n",
    "# Algunas columnas que deberian ser numericas fueron leidas como texto\n",
    "# Se convierten al tipo numerico correcto para poder hacer calculos\n",
    "\n",
    "# Definir las columnas que deben ser numericas\n",
    "columnas_numericas = [\n",
    "    'CANTIDAD DE MUJERES', 'CANTIDAD DE MUJERES EN CARGOS DIRECTIVOS',\n",
    "    '% DE PARTICIPACIÓN DE MUJERES', 'PERSONAL', 'NUMERO DE SOCIOS',\n",
    "    'CANTIDAD DE ESTABLECIMIENTOS', 'CANTIDAD DE AGENCIAS /SUCURSALES',\n",
    "    'PORCENTAJE COMPOSICION DEL CAPITAL NACIONAL PRIVADO',\n",
    "    'PORCENTAJE COMPOSICION DEL CAPITAL NACIONAL PUBLICO',\n",
    "    'PORCENTAJE COMPOSICION DEL CAPITAL NACIONAL TOTAL',\n",
    "    'PORCENTAJE COMPOSICION DEL CAPITAL EXTRANJERO PRIVADO',\n",
    "    'PORCENTAJE COMPOSICION DEL CAPITAL EXTRANJERO PUBLICO\\t',\n",
    "    'PORCENTAJE COMPOSICION DEL CAPITAL EXTRANJERO TOTAL'\n",
    "]\n",
    "\n",
    "for col in columnas_numericas:\n",
    "    if col in df_limpio.columns:\n",
    "        # pd.to_numeric() convierte valores a tipo numerico\n",
    "        # errors='coerce': si un valor no es numerico, lo convierte a NaN\n",
    "        df_limpio[col] = pd.to_numeric(df_limpio[col], errors='coerce')\n",
    "        print(f'Columna \"{col}\" convertida a numerico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 - Limpieza de columnas de texto\n",
    "# Se estandarizan las columnas de texto: eliminar espacios extra y convertir a mayusculas\n",
    "# Esto evita duplicados por diferencias de formato (ej: \"Ibague\" vs \"IBAGUE\" vs \" ibague \")\n",
    "\n",
    "# Seleccionar solo las columnas de tipo objeto (texto)\n",
    "columnas_texto = df_limpio.select_dtypes(include='object').columns\n",
    "\n",
    "for col in columnas_texto:\n",
    "    # .str.strip(): elimina espacios en blanco al inicio y final del texto\n",
    "    # .str.upper(): convierte todo el texto a mayusculas para uniformidad\n",
    "    df_limpio[col] = df_limpio[col].str.strip().str.upper()\n",
    "\n",
    "print(f'Se limpiaron {len(columnas_texto)} columnas de texto (espacios y mayusculas)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7 - Identificar y eliminar columnas con exceso de valores faltantes\n",
    "# Columnas con mas del 80% de datos faltantes aportan poca informacion util\n",
    "\n",
    "# Calcular el porcentaje de nulos por columna\n",
    "porcentaje_nulos = (df_limpio.isna().sum() / len(df_limpio) * 100).sort_values(ascending=False)\n",
    "\n",
    "# Filtrar columnas con mas del 80% de valores nulos\n",
    "columnas_a_eliminar = porcentaje_nulos[porcentaje_nulos > 80].index.tolist()\n",
    "\n",
    "print(f'Columnas con mas del 80% de valores nulos ({len(columnas_a_eliminar)}):') \n",
    "print()\n",
    "for col in columnas_a_eliminar:\n",
    "    print(f'  - {col}: {porcentaje_nulos[col]:.1f}% nulos')\n",
    "\n",
    "# Eliminar esas columnas del DataFrame\n",
    "# axis=1 indica que se eliminan columnas (no filas)\n",
    "df_limpio = df_limpio.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "print(f'\\nColumnas eliminadas: {len(columnas_a_eliminar)}')\n",
    "print(f'Shape despues de eliminar columnas con exceso de nulos: {df_limpio.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.8 - Resumen del estado de los datos despues de la limpieza\n",
    "print('=== RESUMEN POST-LIMPIEZA ===')\n",
    "print(f'Filas: {df_limpio.shape[0]:,}')\n",
    "print(f'Columnas: {df_limpio.shape[1]}')\n",
    "print(f'Valores nulos restantes: {df_limpio.isna().sum().sum():,}')\n",
    "print()\n",
    "print('Tipos de datos:')\n",
    "print(df_limpio.dtypes.value_counts())\n",
    "print()\n",
    "print('Columnas restantes:')\n",
    "for i, col in enumerate(df_limpio.columns, 1):\n",
    "    print(f'  {i:2d}. {col} ({df_limpio[col].dtype})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 5: Analisis de Correlacion y Seleccion de Variables\n",
    "\n",
    "El **analisis de correlacion** permite identificar relaciones lineales entre variables numericas. Se utiliza el coeficiente de **correlacion de Pearson**, que varia entre -1 y 1:\n",
    "\n",
    "- **+1:** Correlacion positiva perfecta (cuando una variable sube, la otra tambien).\n",
    "- **0:** No hay correlacion lineal.\n",
    "- **-1:** Correlacion negativa perfecta (cuando una sube, la otra baja).\n",
    "\n",
    "Este analisis es util para:\n",
    "- Detectar **variables redundantes** (correlacion muy alta entre si, cercana a 1 o -1).\n",
    "- Identificar **variables que no aportan variabilidad** (correlacion de 0 con todas las demas).\n",
    "- Tomar decisiones informadas sobre que columnas conservar o eliminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 5: ANALISIS DE CORRELACION\n",
    "# ============================================================\n",
    "\n",
    "# 5.1 - Seleccionar solo las columnas numericas del DataFrame\n",
    "# .select_dtypes(include='number') filtra columnas de tipo int, float, etc.\n",
    "df_numerico = df_limpio.select_dtypes(include='number')\n",
    "\n",
    "print(f'Variables numericas disponibles: {df_numerico.shape[1]}')\n",
    "print()\n",
    "for i, col in enumerate(df_numerico.columns, 1):\n",
    "    # Mostrar cada variable numerica con su conteo de valores no nulos\n",
    "    no_nulos = df_numerico[col].notna().sum()\n",
    "    print(f'  {i}. {col} ({no_nulos:,} valores validos)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 - Calcular la matriz de correlacion de Pearson\n",
    "# .corr() calcula la correlacion entre todos los pares de columnas numericas\n",
    "# El resultado es una matriz simetrica donde el valor [i,j] es la correlacion entre col_i y col_j\n",
    "matriz_correlacion = df_numerico.corr()\n",
    "\n",
    "# Crear un mapa de calor (heatmap) para visualizar la correlacion\n",
    "# Los colores mas intensos indican correlaciones mas fuertes\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# sns.heatmap() dibuja una matriz coloreada segun los valores\n",
    "# annot=True: muestra los valores numericos en cada celda\n",
    "# fmt='.2f': formato de 2 decimales para los numeros\n",
    "# cmap='coolwarm': paleta de colores azul (negativo) a rojo (positivo)\n",
    "# center=0: el color neutro se centra en correlacion 0\n",
    "# vmin/vmax: rango de la escala de colores\n",
    "# linewidths: grosor de las lineas separadoras entre celdas\n",
    "sns.heatmap(\n",
    "    matriz_correlacion,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    linewidths=0.5,\n",
    "    square=True\n",
    ")\n",
    "\n",
    "plt.title('Matriz de Correlacion de Pearson - Variables Numericas', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nInterpretacion: valores cercanos a 1 o -1 indican correlacion fuerte.')\n",
    "print('Variables con correlacion > 0.9 entre si son candidatas a eliminacion por redundancia.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 - Identificar pares de variables altamente correlacionadas\n",
    "# Se buscan pares con correlacion absoluta > 0.85 (excluyendo la diagonal)\n",
    "\n",
    "# Obtener la parte triangular superior de la matriz para evitar pares duplicados\n",
    "# np.triu crea una mascara triangular superior\n",
    "# k=1 excluye la diagonal principal (correlacion de una variable consigo misma = 1)\n",
    "mascara_superior = np.triu(np.ones_like(matriz_correlacion, dtype=bool), k=1)\n",
    "\n",
    "# Aplicar la mascara y convertir a formato largo (cada fila es un par de variables)\n",
    "# .stack() convierte la matriz en una serie con multi-indice\n",
    "correlaciones_altas = (\n",
    "    matriz_correlacion.where(mascara_superior)\n",
    "    .stack()\n",
    "    .reset_index()\n",
    ")\n",
    "correlaciones_altas.columns = ['Variable_1', 'Variable_2', 'Correlacion']\n",
    "\n",
    "# Filtrar solo las correlaciones con valor absoluto mayor a 0.85\n",
    "correlaciones_altas = correlaciones_altas[\n",
    "    correlaciones_altas['Correlacion'].abs() > 0.85\n",
    "].sort_values('Correlacion', ascending=False)\n",
    "\n",
    "if len(correlaciones_altas) > 0:\n",
    "    print('Pares de variables con correlacion > 0.85:')\n",
    "    print()\n",
    "    for _, row in correlaciones_altas.iterrows():\n",
    "        print(f'  {row[\"Variable_1\"]}  <->  {row[\"Variable_2\"]}  =  {row[\"Correlacion\"]:.3f}')\n",
    "else:\n",
    "    print('No se encontraron pares de variables con correlacion > 0.85')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 - Eliminar variables redundantes basado en el analisis de correlacion\n",
    "# Cuando dos variables tienen correlacion muy alta (>0.85), se elimina una de ellas\n",
    "# Se conserva la que tiene un nombre mas descriptivo o mayor utilidad analitica\n",
    "\n",
    "# Identificar columnas redundantes para eliminar\n",
    "# Criterio: de cada par altamente correlacionado, eliminar la menos informativa\n",
    "columnas_redundantes = set()\n",
    "\n",
    "for _, row in correlaciones_altas.iterrows():\n",
    "    var1, var2 = row['Variable_1'], row['Variable_2']\n",
    "    # Si ambas son de porcentaje de capital, conservar el total y eliminar los parciales\n",
    "    if 'TOTAL' in str(var2):\n",
    "        columnas_redundantes.add(var1)\n",
    "    elif 'TOTAL' in str(var1):\n",
    "        columnas_redundantes.add(var2)\n",
    "    else:\n",
    "        # Por defecto, eliminar la segunda variable del par\n",
    "        columnas_redundantes.add(var2)\n",
    "\n",
    "# Eliminar las columnas redundantes si existen en el DataFrame\n",
    "columnas_a_quitar = [col for col in columnas_redundantes if col in df_limpio.columns]\n",
    "\n",
    "if columnas_a_quitar:\n",
    "    df_limpio = df_limpio.drop(columns=columnas_a_quitar)\n",
    "    print(f'Columnas eliminadas por redundancia ({len(columnas_a_quitar)}):') \n",
    "    for col in columnas_a_quitar:\n",
    "        print(f'  - {col}')\n",
    "else:\n",
    "    print('No se eliminaron columnas por redundancia.')\n",
    "\n",
    "print(f'\\nShape final despues de seleccion de variables: {df_limpio.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 6: Transformaciones Adicionales\n",
    "\n",
    "En este paso se realizan transformaciones que agregan valor analitico al dataset:\n",
    "\n",
    "1. **Extraccion del codigo de municipio:** Se separa el codigo numerico del nombre del municipio para facilitar agrupaciones.\n",
    "2. **Calculo de antiguedad de la empresa:** Se calcula cuantos anos lleva registrada cada empresa desde su fecha de matricula.\n",
    "3. **Clasificacion del CIIU:** Se extrae el codigo de actividad economica principal para analisis sectorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 6: TRANSFORMACIONES ADICIONALES\n",
    "# ============================================================\n",
    "\n",
    "# 6.1 - Extraer el nombre del municipio sin el codigo\n",
    "# El formato original es \"73001 - IBAGUE\", se extrae solo \"IBAGUE\"\n",
    "\n",
    "if 'MUNICIPIO COMERCIAL' in df_limpio.columns:\n",
    "    # .str.split(' - '): divide el texto por el separador \" - \"\n",
    "    # .str[-1]: toma la ultima parte (el nombre del municipio)\n",
    "    # .str.strip(): elimina espacios sobrantes\n",
    "    df_limpio['NOMBRE_MUNICIPIO'] = (\n",
    "        df_limpio['MUNICIPIO COMERCIAL']\n",
    "        .str.split(' - ')\n",
    "        .str[-1]\n",
    "        .str.strip()\n",
    "    )\n",
    "    print('Columna NOMBRE_MUNICIPIO creada exitosamente.')\n",
    "    print(f'Municipios unicos: {df_limpio[\"NOMBRE_MUNICIPIO\"].nunique()}')\n",
    "    print()\n",
    "    # Mostrar los 10 municipios con mas empresas\n",
    "    print('Top 10 municipios por cantidad de empresas:')\n",
    "    print(df_limpio['NOMBRE_MUNICIPIO'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 - Calcular la antiguedad de cada empresa (en anos)\n",
    "# Se calcula como la diferencia entre la fecha de corte (31-dic-2025) y la fecha de matricula\n",
    "\n",
    "if 'FECHA DE MATRICULA' in df_limpio.columns:\n",
    "    # Definir la fecha de referencia (corte del dataset)\n",
    "    fecha_corte = pd.Timestamp('2025-12-31')\n",
    "    \n",
    "    # Calcular la diferencia en dias y convertir a anos\n",
    "    # .dt.days: extrae la cantidad de dias de la diferencia temporal\n",
    "    # / 365.25: convierte dias a anos (considerando anos bisiestos)\n",
    "    df_limpio['ANTIGUEDAD_ANOS'] = (\n",
    "        (fecha_corte - df_limpio['FECHA DE MATRICULA']).dt.days / 365.25\n",
    "    ).round(1)\n",
    "    \n",
    "    print('Columna ANTIGUEDAD_ANOS creada exitosamente.')\n",
    "    print()\n",
    "    print('Estadisticas de antiguedad (anos):')\n",
    "    print(df_limpio['ANTIGUEDAD_ANOS'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 - Extraer el codigo CIIU principal (sector economico)\n",
    "# El formato original es \"S9499 ** Actividades de otras asociaciones n.c.p.\"\n",
    "# Se extrae solo el codigo \"S9499\" para facilitar agrupaciones por sector\n",
    "\n",
    "if 'CIIU-1' in df_limpio.columns:\n",
    "    # .str.split(' '): divide por espacios\n",
    "    # .str[0]: toma la primera parte (el codigo CIIU)\n",
    "    df_limpio['CIIU_CODIGO'] = (\n",
    "        df_limpio['CIIU-1']\n",
    "        .str.split(' ')\n",
    "        .str[0]\n",
    "    )\n",
    "    \n",
    "    # Extraer la letra inicial del CIIU que indica la seccion economica\n",
    "    # Ejemplo: S = Otras actividades de servicios, G = Comercio, C = Manufactura\n",
    "    df_limpio['SECCION_ECONOMICA'] = df_limpio['CIIU_CODIGO'].str[0]\n",
    "    \n",
    "    print('Columnas CIIU_CODIGO y SECCION_ECONOMICA creadas exitosamente.')\n",
    "    print()\n",
    "    print('Top 10 secciones economicas:')\n",
    "    print(df_limpio['SECCION_ECONOMICA'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 7: Analisis Exploratorio de Datos (EDA)\n",
    "\n",
    "El **Analisis Exploratorio de Datos (EDA)** tiene como objetivo descubrir patrones, tendencias y anomalias en los datos mediante tecnicas visuales y estadisticas.\n",
    "\n",
    "En esta seccion se presentan tres tipos de visualizaciones:\n",
    "\n",
    "1. **Histogramas:** Permiten observar la distribucion de frecuencias de variables numericas.\n",
    "2. **Nube de palabras:** Muestra visualmente las palabras mas frecuentes en los nombres de las empresas.\n",
    "3. **Grafico de barras:** Presenta la distribucion de empresas por tipo de organizacion y tamano empresarial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Histogramas\n",
    "\n",
    "Un **histograma** es un grafico que representa la distribucion de frecuencias de una variable numerica continua. El eje X muestra los rangos de valores (bins) y el eje Y muestra cuantos registros caen en cada rango.\n",
    "\n",
    "Se generan histogramas para:\n",
    "- **Antiguedad de las empresas:** Para ver cuantas empresas son nuevas vs. antiguas.\n",
    "- **Cantidad de mujeres en las empresas:** Para analizar la participacion femenina.\n",
    "- **Numero de socios:** Para entender la estructura societaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 7.1: HISTOGRAMAS\n",
    "# ============================================================\n",
    "\n",
    "# Crear una figura con 3 subgraficos dispuestos en una fila\n",
    "# figsize=(18, 5): ancho de 18 y alto de 5 pulgadas\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# --- Histograma 1: Distribucion de la Antiguedad ---\n",
    "if 'ANTIGUEDAD_ANOS' in df_limpio.columns:\n",
    "    # .dropna(): elimina valores nulos antes de graficar\n",
    "    datos_antiguedad = df_limpio['ANTIGUEDAD_ANOS'].dropna()\n",
    "    \n",
    "    # axes[0].hist(): dibuja el histograma en el primer subgrafico\n",
    "    # bins=30: divide los datos en 30 intervalos\n",
    "    # color: color de las barras\n",
    "    # edgecolor: color del borde de las barras\n",
    "    # alpha: transparencia (0=transparente, 1=opaco)\n",
    "    axes[0].hist(datos_antiguedad, bins=30, color='#2196F3', edgecolor='white', alpha=0.85)\n",
    "    axes[0].set_title('Distribucion de Antiguedad\\nde las Empresas', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Antiguedad (anos)', fontsize=10)\n",
    "    axes[0].set_ylabel('Frecuencia (cantidad de empresas)', fontsize=10)\n",
    "    # Agregar linea vertical para la mediana\n",
    "    mediana_ant = datos_antiguedad.median()\n",
    "    axes[0].axvline(mediana_ant, color='red', linestyle='--', linewidth=1.5, label=f'Mediana: {mediana_ant:.1f} anos')\n",
    "    axes[0].legend(fontsize=9)\n",
    "\n",
    "# --- Histograma 2: Cantidad de Mujeres ---\n",
    "if 'CANTIDAD DE MUJERES' in df_limpio.columns:\n",
    "    datos_mujeres = df_limpio['CANTIDAD DE MUJERES'].dropna()\n",
    "    # Filtrar valores extremos para mejor visualizacion (percentil 99)\n",
    "    limite_superior = datos_mujeres.quantile(0.99)\n",
    "    datos_mujeres_filtrado = datos_mujeres[datos_mujeres <= limite_superior]\n",
    "    \n",
    "    axes[1].hist(datos_mujeres_filtrado, bins=25, color='#E91E63', edgecolor='white', alpha=0.85)\n",
    "    axes[1].set_title('Distribucion de Cantidad\\nde Mujeres por Empresa', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Cantidad de mujeres', fontsize=10)\n",
    "    axes[1].set_ylabel('Frecuencia (cantidad de empresas)', fontsize=10)\n",
    "    mediana_muj = datos_mujeres_filtrado.median()\n",
    "    axes[1].axvline(mediana_muj, color='red', linestyle='--', linewidth=1.5, label=f'Mediana: {mediana_muj:.0f}')\n",
    "    axes[1].legend(fontsize=9)\n",
    "\n",
    "# --- Histograma 3: Numero de Socios ---\n",
    "if 'NUMERO DE SOCIOS' in df_limpio.columns:\n",
    "    datos_socios = df_limpio['NUMERO DE SOCIOS'].dropna()\n",
    "    # Filtrar valores extremos\n",
    "    limite_socios = datos_socios.quantile(0.99)\n",
    "    datos_socios_filtrado = datos_socios[datos_socios <= limite_socios]\n",
    "    \n",
    "    axes[2].hist(datos_socios_filtrado, bins=25, color='#4CAF50', edgecolor='white', alpha=0.85)\n",
    "    axes[2].set_title('Distribucion del Numero\\nde Socios por Empresa', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_xlabel('Numero de socios', fontsize=10)\n",
    "    axes[2].set_ylabel('Frecuencia (cantidad de empresas)', fontsize=10)\n",
    "    mediana_soc = datos_socios_filtrado.median()\n",
    "    axes[2].axvline(mediana_soc, color='red', linestyle='--', linewidth=1.5, label=f'Mediana: {mediana_soc:.0f}')\n",
    "    axes[2].legend(fontsize=9)\n",
    "\n",
    "# Ajustar el espaciado entre subgraficos para evitar solapamiento\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nInterpretacion: Los histogramas muestran la distribucion de frecuencias.')\n",
    "print('Las lineas rojas punteadas indican la mediana de cada variable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Nube de Palabras\n",
    "\n",
    "Una **nube de palabras (Word Cloud)** es una representacion visual donde el tamano de cada palabra es proporcional a su frecuencia de aparicion en el texto analizado. Las palabras mas grandes son las que aparecen con mayor frecuencia.\n",
    "\n",
    "Se genera una nube de palabras a partir de los **nombres de las empresas (RAZON SOCIAL)** para identificar:\n",
    "- Sectores economicos predominantes en la region.\n",
    "- Tipos de organizaciones mas comunes.\n",
    "- Palabras clave que caracterizan el tejido empresarial de Ibague."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 7.2: NUBE DE PALABRAS\n",
    "# ============================================================\n",
    "\n",
    "if 'RAZON SOCIAL' in df_limpio.columns:\n",
    "    # Obtener todos los nombres de empresas, eliminando valores nulos\n",
    "    nombres_empresas = df_limpio['RAZON SOCIAL'].dropna()\n",
    "    \n",
    "    # Unir todos los nombres en un solo texto largo separado por espacios\n",
    "    # Esto es necesario porque WordCloud espera un unico string como entrada\n",
    "    texto_completo = ' '.join(nombres_empresas)\n",
    "    \n",
    "    # Definir palabras a excluir (stopwords)\n",
    "    # Son palabras comunes que no aportan informacion relevante al analisis\n",
    "    stopwords_es = {\n",
    "        'DE', 'LA', 'EL', 'EN', 'LOS', 'LAS', 'DEL', 'Y', 'E', 'A',\n",
    "        'POR', 'PARA', 'CON', 'SIN', 'SU', 'AL', 'SE', 'NO', 'QUE',\n",
    "        'UN', 'UNA', 'ES', 'SAS', 'SA', 'LTDA', 'S', 'NRO', 'N',\n",
    "        'O', 'AS', 'EU', 'CI'\n",
    "    }\n",
    "    \n",
    "    # Crear el objeto WordCloud con configuraciones personalizadas\n",
    "    # width/height: dimensiones de la imagen en pixeles\n",
    "    # max_words: maximo de palabras a incluir\n",
    "    # background_color: color de fondo\n",
    "    # colormap: paleta de colores para las palabras\n",
    "    # min_font_size: tamano minimo de fuente\n",
    "    # stopwords: palabras a excluir\n",
    "    nube = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        max_words=150,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        min_font_size=8,\n",
    "        stopwords=stopwords_es,\n",
    "        collocations=False  # Evitar repetir combinaciones de palabras\n",
    "    ).generate(texto_completo)\n",
    "    \n",
    "    # Mostrar la nube de palabras\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # imshow() muestra una imagen (la nube de palabras generada)\n",
    "    # interpolation='bilinear': suaviza los bordes de las letras\n",
    "    plt.imshow(nube, interpolation='bilinear')\n",
    "    \n",
    "    # Ocultar los ejes X e Y ya que no aportan informacion en una nube de palabras\n",
    "    plt.axis('off')\n",
    "    plt.title('Nube de Palabras - Nombres de Empresas (Razon Social)\\nCamara de Comercio de Ibague',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('Las palabras mas grandes representan los terminos mas frecuentes en los nombres de las empresas.')\n",
    "    print('Esto permite identificar los sectores y tipos de organizacion predominantes en la region.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Grafico de Barras: Distribucion por Tipo de Organizacion y Tamano Empresarial\n",
    "\n",
    "Un **grafico de barras** muestra la frecuencia o conteo de categorias discretas. Es ideal para comparar cantidades entre grupos diferentes.\n",
    "\n",
    "Se generan dos graficos de barras:\n",
    "1. **Por tipo de organizacion:** Muestra cuantas empresas son personas naturales, sociedades, ESAL, etc.\n",
    "2. **Por tamano empresarial:** Muestra la distribucion entre micro, pequenas, medianas y grandes empresas.\n",
    "\n",
    "Estos graficos permiten comprender la composicion del tejido empresarial de Ibague."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 7.3: GRAFICOS DE BARRAS\n",
    "# ============================================================\n",
    "\n",
    "# Crear una figura con 2 subgraficos lado a lado\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Grafico de Barras 1: Tipo de Organizacion ---\n",
    "if 'ORGANIZACION' in df_limpio.columns:\n",
    "    # .value_counts(): cuenta cuantas veces aparece cada valor unico\n",
    "    # Devuelve una serie ordenada de mayor a menor frecuencia\n",
    "    conteo_org = df_limpio['ORGANIZACION'].value_counts()\n",
    "    \n",
    "    # Definir colores personalizados para cada barra\n",
    "    colores_org = ['#2196F3', '#4CAF50', '#FF9800', '#9C27B0', '#F44336']\n",
    "    \n",
    "    # .plot(kind='bar'): crea un grafico de barras verticales\n",
    "    # ax=axes[0]: lo dibuja en el primer subgrafico\n",
    "    conteo_org.plot(\n",
    "        kind='bar',\n",
    "        ax=axes[0],\n",
    "        color=colores_org[:len(conteo_org)],\n",
    "        edgecolor='white',\n",
    "        width=0.7\n",
    "    )\n",
    "    axes[0].set_title('Distribucion por Tipo\\nde Organizacion', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Tipo de Organizacion', fontsize=10)\n",
    "    axes[0].set_ylabel('Cantidad de Empresas', fontsize=10)\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Agregar etiquetas de valor encima de cada barra\n",
    "    for i, (valor, nombre) in enumerate(zip(conteo_org.values, conteo_org.index)):\n",
    "        # .text(): agrega texto en una posicion especifica del grafico\n",
    "        axes[0].text(i, valor + 100, f'{valor:,}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# --- Grafico de Barras 2: Tamano Empresarial ---\n",
    "if 'TAMAÑO EMPRESA' in df_limpio.columns:\n",
    "    conteo_tamano = df_limpio['TAMAÑO EMPRESA'].value_counts()\n",
    "    \n",
    "    colores_tam = ['#00BCD4', '#8BC34A', '#FFC107', '#FF5722', '#673AB7']\n",
    "    \n",
    "    conteo_tamano.plot(\n",
    "        kind='bar',\n",
    "        ax=axes[1],\n",
    "        color=colores_tam[:len(conteo_tamano)],\n",
    "        edgecolor='white',\n",
    "        width=0.7\n",
    "    )\n",
    "    axes[1].set_title('Distribucion por Tamano\\nEmpresarial', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Tamano Empresarial', fontsize=10)\n",
    "    axes[1].set_ylabel('Cantidad de Empresas', fontsize=10)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Agregar etiquetas de valor encima de cada barra\n",
    "    for i, (valor, nombre) in enumerate(zip(conteo_tamano.values, conteo_tamano.index)):\n",
    "        axes[1].text(i, valor + 100, f'{valor:,}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nInterpretacion:')\n",
    "print('- El grafico izquierdo muestra la proporcion de empresas segun su tipo juridico.')\n",
    "print('- El grafico derecho muestra como se distribuyen las empresas segun su tamano.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 8: Carga de Datos (Load) - Exportacion\n",
    "\n",
    "La fase final del proceso ETL es la **carga (Load)**, que consiste en almacenar los datos ya limpios y transformados en un formato reutilizable.\n",
    "\n",
    "Se exporta el DataFrame limpio a un archivo CSV que puede ser utilizado en analisis posteriores, dashboards, o modelos de machine learning sin necesidad de repetir el proceso de limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 8: CARGA DE DATOS (LOAD) - EXPORTACION\n",
    "# ============================================================\n",
    "\n",
    "# Definir la ruta de salida para el archivo limpio\n",
    "ruta_salida = r'data/empresas_ibague_limpio.csv'\n",
    "\n",
    "# .to_csv(): exporta el DataFrame a un archivo CSV\n",
    "# index=False: no incluir el indice numerico de pandas como columna en el CSV\n",
    "# encoding='utf-8-sig': usar UTF-8 con BOM para compatibilidad con Excel en Windows\n",
    "df_limpio.to_csv(ruta_salida, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print('=== EXPORTACION COMPLETADA ===')\n",
    "print(f'Archivo guardado en: {ruta_salida}')\n",
    "print(f'Filas exportadas: {df_limpio.shape[0]:,}')\n",
    "print(f'Columnas exportadas: {df_limpio.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 9: Resumen y Conclusiones\n",
    "\n",
    "A continuacion se presenta un resumen de todo el proceso ETL realizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PASO 9: RESUMEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 60)\n",
    "print('       RESUMEN DEL PROCESO ETL')\n",
    "print('       Camara de Comercio de Ibague')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print(f'  Dataset original:     {df.shape[0]:,} filas x {df.shape[1]} columnas')\n",
    "print(f'  Dataset final:        {df_limpio.shape[0]:,} filas x {df_limpio.shape[1]} columnas')\n",
    "print(f'  Columnas eliminadas:  {df.shape[1] - df_limpio.shape[1] + 3}')  # +3 por las columnas nuevas\n",
    "print(f'  Columnas agregadas:   3 (NOMBRE_MUNICIPIO, ANTIGUEDAD_ANOS, CIIU_CODIGO)')\n",
    "print()\n",
    "print('  Transformaciones realizadas:')\n",
    "print('    1. Reemplazo de \"No reporta\" y \"No aplica\" por NaN')\n",
    "print('    2. Eliminacion de filas duplicadas')\n",
    "print('    3. Conversion de columnas de fecha (YYYYMMDD -> datetime)')\n",
    "print('    4. Conversion de columnas numericas (texto -> numerico)')\n",
    "print('    5. Estandarizacion de texto (mayusculas, sin espacios extra)')\n",
    "print('    6. Eliminacion de columnas con >80% de valores nulos')\n",
    "print('    7. Eliminacion de columnas redundantes por correlacion')\n",
    "print('    8. Creacion de variables derivadas (municipio, antiguedad, CIIU)')\n",
    "print()\n",
    "print('  Visualizaciones generadas:')\n",
    "print('    - Mapa de calor de correlacion')\n",
    "print('    - Histogramas de antiguedad, mujeres y socios')\n",
    "print('    - Nube de palabras de razones sociales')\n",
    "print('    - Graficos de barras de organizacion y tamano empresarial')\n",
    "print()\n",
    "print(f'  Archivo exportado: data/empresas_ibague_limpio.csv')\n",
    "print()\n",
    "print('=' * 60)\n",
    "print('  Autor: Juan Camilo Perea Possos')\n",
    "print('  Fecha: 20 de Febrero de 2026')\n",
    "print('=' * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
